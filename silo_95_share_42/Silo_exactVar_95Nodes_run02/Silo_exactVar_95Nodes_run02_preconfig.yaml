# An example YAML file

---

# Time span that the simulation will run.
#timespan: 21941800 # for dft all data
#timespan: 17362800 # for pit all data
#timespan: 289400 # for pit bigData
#timespan: 158867000 # 5 years of HPC data
#timespan: 950400 # testing 11 days of HPC data
#timespan: 15552000 # testing 180 days of HPC data
#timespan: 54000 # testing 6to20 hour of HPC data

#timespan: 366000 # seconds in 4.2 days
# parameters for initializing the HPC cluster
#
data_collection_interval: &dci 1 # interval in seconds at which to collect state of the simulation. 
ALL:
  # Set all clouds except the main cluster to cluster_size = 15 
  exp_name: &exp_name 'Silo_exactVar_95Nodes_run02'
  silo: True
  # exp_name: &exp_name 'silo_01_clusters_11'
  tracetime: 36000 # stop reading new inputs from trace files after this point in time. 
  runtime: 43200 # Model time span. 12 hours (in seconds)
  dir_input: &inputdir 'input_files' # in pwd
  dir_validate: &validdir [*inputdir, 'validation_parameters']
  validate: &val True
  path_logs: &logpath ['logs', *exp_name] # in pwd
  contract_lease_duration: &leasetime 600  # 5 minutes in seconds
  tracefiles:
    hpc: &hpc_input 'HPC_model_input_trace.csv'
    #cloud: &cld_input 'model450_rev_alt_peak_cloud_input.csv'
    cloud50: &cld50_input 'cloud50_input.csv'
    bigdata: &bgd_input 'bgd_input_same_as_exp.csv'
  clusters: [
  'hpc_main',
  'bigdata_main',
  'cloud50',
  ]
  node_transfer_delay: 300
    
marketplace:
  name: M1
  auction_begins_at: 1800   # Timestamp in seconds when 1st auction will commence.
  lease_duration: *leasetime   # Lease in seconds. eg. 30 min lease = 1800 seconds
  path_output: *logpath
  stats_auction: 'all_auctions.csv'

  
  # A contract will last for this many seconds.
  # Also, every subsequent auction repeats after this time.x

hpc_main:
  name: hpc_main
  type: hpc
  nodes: 49      # Number of nodes owned by the cluster
  cores_per_node: 24    # Cores per node in the cluster. Assumes homogeneous hardware. 
  tracetime: 36000
  path_input: [ *inputdir, *hpc_input ]
  validate: *val
  silo_validation: [ *validdir, {'end_delay':'slurm_enddelay_variations.csv'} ]
  path_output: *logpath
  report_interval: *dci
  agent:
    type: hpc
    budget: 400

cloud50:
  name: cloud50
  type: cloud
  nodes: 20
  cores_per_node: 24
  tracetime: 36000 
  path_input: [ *inputdir, *cld50_input ]
  validate: *val
  silo_validation: [ *validdir, {
  'start_delay': 'ostack_time2selectHypervisor_Allvariations.csv',
  'end_delay': 'ostack_time2end_variations.csv'
  } ]
  path_output: *logpath
  report_interval: *dci
  agent:
    type: cloud
    budget: 400
    # Only if cluster type is cloud:
    history:
      windowsize: 6
      update_interval: 100 # time in seconds

bigdata_main:
  name: bigdata_main
  type: bigdata
  nodes: 26
  cores_per_node: 24
  path_input: [ *inputdir, *bgd_input ]
  tracetime: 36000
  validate: *val
  silo_validation: [ *validdir, {
  'submit_delay': 'spark_submitdelay_variations.csv',
  'start_delay':'spark_startdelay_variations.csv',
  'end_delay':'spark_enddelay_variations.csv'
  } ]
  path_output: *logpath
  report_interval: *dci
  agent:
    type: bigdata
    budget: 400


